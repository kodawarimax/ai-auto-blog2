"""
å®Œå…¨è‡ªå‹•AIæŠ•ç¨¿ã‚·ã‚¹ãƒ†ãƒ  - å¼·åŒ–ç‰ˆãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ç¢ºå®Ÿã«AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
"""
import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime, timedelta
import time
import hashlib
from typing import List, Dict, Optional
import re


class EnhancedNewsCollector:
    """å¼·åŒ–ç‰ˆãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """å„ç¨®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‚’è¨­å®š"""
        # RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆæœ€ã‚‚ç¢ºå®Ÿãªæ–¹æ³•ï¼‰
        self.rss_feeds = [
            {
                'name': 'Google News - AI',
                'url': 'https://news.google.com/rss/search?q=AI+äººå·¥çŸ¥èƒ½&hl=ja&gl=JP&ceid=JP:ja',
                'type': 'rss'
            },
            {
                'name': 'ITmedia AI+',
                'url': 'https://rss.itmedia.co.jp/rss/2.0/aiplus.xml',
                'type': 'rss'
            },
            {
                'name': 'GIGAZINE',
                'url': 'https://gigazine.net/news/rss_2.0/',
                'type': 'rss',
                'filter': ['AI', 'äººå·¥çŸ¥èƒ½', 'ChatGPT', 'æ©Ÿæ¢°å­¦ç¿’']
            }
        ]
        
        # ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”¨ã®ã‚½ãƒ¼ã‚¹
        self.web_sources = [
            {
                'name': 'AI News Japan',
                'url': 'https://ledge.ai/categories/news/',
                'selector': {
                    'container': 'article',
                    'title': 'h2',
                    'link': 'a',
                    'summary': 'p'
                }
            },
            {
                'name': 'ASCII AI',
                'url': 'https://ascii.jp/ai/',
                'selector': {
                    'container': 'div.articleList',
                    'title': 'h3',
                    'link': 'a',
                    'summary': 'p'
                }
            }
        ]
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã®ãµã‚Šã‚’ã™ã‚‹ï¼‰
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        
        # åé›†ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¸€æ™‚ä¿å­˜ï¼ˆé‡è¤‡é˜²æ­¢ç”¨ï¼‰
        self.collected_news = []
        self.seen_urls = set()
    
    def get_ai_news(self, limit: int = 5, hours_back: int = 48) -> List[Dict]:
        """
        AIé–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
        
        Args:
            limit: å–å¾—ã™ã‚‹è¨˜äº‹æ•°
            hours_back: ä½•æ™‚é–“å‰ã¾ã§ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã™ã‚‹ã‹
        
        Returns:
            ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        """
        print(f"ğŸ“¡ {hours_back}æ™‚é–“ä»¥å†…ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†é–‹å§‹...")
        
        # æœŸé™ã‚’è¨­å®š
        time_limit = datetime.now() - timedelta(hours=hours_back)
        
        # 1. RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰åé›†ï¼ˆæœ€ã‚‚ç¢ºå®Ÿï¼‰
        self._collect_from_rss(time_limit)
        
        # 2. ä¸è¶³åˆ†ã‚’ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§è£œå®Œ
        if len(self.collected_news) < limit:
            self._collect_from_web()
        
        # 3. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æº–å‚™
        if len(self.collected_news) < limit:
            self._add_fallback_news()
        
        # 4. é‡è¤‡ã‚’é™¤å»ã—ã¦æœ€æ–°é †ã«ã‚½ãƒ¼ãƒˆ
        unique_news = self._remove_duplicates()
        
        # 5. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¦é‡è¦åº¦é †ã«ä¸¦ã¹æ›¿ãˆ
        scored_news = self._score_and_sort(unique_news)
        
        # 6. æŒ‡å®šæ•°ã ã‘è¿”ã™
        result = scored_news[:limit]
        
        print(f"âœ… {len(result)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†å®Œäº†")
        return result
    
    def _collect_from_rss(self, time_limit: datetime):
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        for feed_info in self.rss_feeds:
            try:
                print(f"  ğŸ“» {feed_info['name']}ã‹ã‚‰åé›†ä¸­...")
                
                # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
                feed = feedparser.parse(feed_info['url'])
                
                if not feed.entries:
                    continue
                
                # å„è¨˜äº‹ã‚’å‡¦ç†
                for entry in feed.entries[:10]:  # æœ€æ–°10ä»¶ã¾ã§
                    # å…¬é–‹æ—¥æ™‚ã‚’ç¢ºèª
                    published = self._parse_date(entry.get('published', ''))
                    if published and published < time_limit:
                        continue
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒã‚ã‚‹å ´åˆã¯é©ç”¨
                    if 'filter' in feed_info:
                        title = entry.get('title', '')
                        if not any(keyword in title for keyword in feed_info['filter']):
                            continue
                    
                    # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ä½œæˆ
                    article = {
                        'title': self._clean_text(entry.get('title', '')),
                        'url': entry.get('link', ''),
                        'summary': self._clean_text(entry.get('summary', '')[:200]),
                        'source': feed_info['name'],
                        'published': published.isoformat() if published else datetime.now().isoformat(),
                        'score': 0  # å¾Œã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
                    }
                    
                    # URLã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if article['url'] and article['url'] not in self.seen_urls:
                        self.collected_news.append(article)
                        self.seen_urls.add(article['url'])
                
                time.sleep(0.5)  # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã
                
            except Exception as e:
                print(f"    âš ï¸ {feed_info['name']}ã®RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _collect_from_web(self):
        """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰ç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        for source in self.web_sources:
            try:
                print(f"  ğŸŒ {source['name']}ã‹ã‚‰åé›†ä¸­...")
                
                # ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‚’å–å¾—
                response = requests.get(source['url'], headers=self.headers, timeout=10)
                if response.status_code != 200:
                    continue
                
                # BeautifulSoupã§è§£æ
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã«åŸºã¥ã„ã¦è¨˜äº‹ã‚’æŠ½å‡º
                selector = source['selector']
                containers = soup.select(selector['container'])[:5]  # æœ€æ–°5ä»¶
                
                for container in containers:
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    title_elem = container.select_one(selector['title'])
                    if not title_elem:
                        continue
                    
                    title = self._clean_text(title_elem.get_text())
                    
                    # AIé–¢é€£ã®è¨˜äº‹ã‹ãƒã‚§ãƒƒã‚¯
                    if not self._is_ai_related(title):
                        continue
                    
                    # ãƒªãƒ³ã‚¯ã‚’å–å¾—
                    link_elem = container.select_one(selector['link'])
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = source['url'].split('/')[0] + '//' + source['url'].split('/')[2] + url
                    
                    # æ¦‚è¦ã‚’å–å¾—
                    summary_elem = container.select_one(selector.get('summary', 'p'))
                    summary = self._clean_text(summary_elem.get_text()[:200]) if summary_elem else ''
                    
                    # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ä½œæˆ
                    article = {
                        'title': title,
                        'url': url,
                        'summary': summary or f'{title}ã«é–¢ã™ã‚‹æœ€æ–°æƒ…å ±ã§ã™ã€‚',
                        'source': source['name'],
                        'published': datetime.now().isoformat(),
                        'score': 0
                    }
                    
                    # URLã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if url and url not in self.seen_urls:
                        self.collected_news.append(article)
                        self.seen_urls.add(url)
                
                time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã
                
            except Exception as e:
                print(f"    âš ï¸ {source['name']}ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _add_fallback_news(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¿½åŠ """
        fallback_news = [
            {
                'title': 'ã€æœ€æ–°ã€‘ç”ŸæˆAIãŒå¤‰ãˆã‚‹ç§ãŸã¡ã®æœªæ¥',
                'summary': 'ChatGPTã‚„Geminiãªã©ã®ç”ŸæˆAIãŒã€æ•™è‚²ã‚„ãƒ“ã‚¸ãƒã‚¹ã®ç¾å ´ã§æ€¥é€Ÿã«æ™®åŠã—ã¦ã„ã¾ã™ã€‚AIã¨ã®å…±å­˜ã«ã¤ã„ã¦è€ƒãˆã‚‹æ™‚ãŒæ¥ã¦ã„ã¾ã™ã€‚',
                'url': 'https://example.com/ai-future',
                'source': 'AI Times (Fallback)',
                'published': datetime.now().isoformat(),
                'score': 0
            },
            {
                'title': 'AIæŠ€è¡“ã®å€«ç†çš„ãªèª²é¡Œã¨è§£æ±ºç­–',
                'summary': 'AIæŠ€è¡“ã®ç™ºå±•ã«ä¼´ã„ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚„è‘—ä½œæ¨©ãªã©ã®å€«ç†çš„ãªèª²é¡ŒãŒæµ®ä¸Šã—ã¦ã„ã¾ã™ã€‚é©åˆ‡ãªãƒ«ãƒ¼ãƒ«ä½œã‚ŠãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚',
                'url': 'https://example.com/ai-ethics',
                'source': 'Tech Ethics (Fallback)',
                'published': datetime.now().isoformat(),
                'score': 0
            },
            {
                'title': 'æ—¥æœ¬ä¼æ¥­ã®AIæ´»ç”¨äº‹ä¾‹10é¸',
                'summary': 'è£½é€ æ¥­ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹æ¥­ã¾ã§ã€æ§˜ã€…ãªåˆ†é‡ã§AIãŒæ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚æˆåŠŸäº‹ä¾‹ã‹ã‚‰å­¦ã¶AIå°å…¥ã®ãƒã‚¤ãƒ³ãƒˆã‚’ç´¹ä»‹ã—ã¾ã™ã€‚',
                'url': 'https://example.com/ai-cases',
                'source': 'Business AI (Fallback)',
                'published': datetime.now().isoformat(),
                'score': 0
            }
        ]
        
        for article in fallback_news:
            if article['url'] not in self.seen_urls:
                self.collected_news.append(article)
                self.seen_urls.add(article['url'])
    
    def _is_ai_related(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒAIé–¢é€£ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        ai_keywords = [
            'AI', 'äººå·¥çŸ¥èƒ½', 'ChatGPT', 'GPT', 'Gemini', 'Claude',
            'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'æ·±å±¤å­¦ç¿’', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ',
            'ç”ŸæˆAI', 'LLM', 'å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«', 'ç”»åƒç”Ÿæˆ', 'éŸ³å£°èªè­˜',
            'OpenAI', 'Google AI', 'Microsoft AI', 'Meta AI',
            'Stable Diffusion', 'DALL-E', 'Midjourney',
            'AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ', 'ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ', 'è‡ªå‹•åŒ–', 'ãƒ­ãƒœãƒƒãƒˆ'
        ]
        
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in ai_keywords)
    
    def _clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # HTMLã‚¿ã‚°ã‚’é™¤å»
        text = re.sub(r'<[^>]+>', '', text)
        # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
        text = re.sub(r'\s+', ' ', text)
        # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
        text = text.strip()
        # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
        text = text.replace('\u3000', ' ')  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹
        text = text.replace('\xa0', ' ')    # ãƒãƒ¼ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
        
        return text
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        if not date_str:
            return None
        
        # ä¸€èˆ¬çš„ãªæ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è©¦ã™
        date_formats = [
            '%a, %d %b %Y %H:%M:%S %Z',  # RFC822
            '%Y-%m-%dT%H:%M:%S%z',       # ISO8601
            '%Y-%m-%d %H:%M:%S',         # ä¸€èˆ¬çš„ãªå½¢å¼
            '%Y/%m/%d %H:%M:%S',         # æ—¥æœ¬å¼
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.replace('GMT', '+0000'), fmt)
            except:
                continue
        
        return None
    
    def _remove_duplicates(self) -> List[Dict]:
        """é‡è¤‡è¨˜äº‹ã‚’é™¤å»"""
        unique_news = []
        seen_titles = set()
        
        for article in self.collected_news:
            # ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼æ€§ã§ã‚‚é‡è¤‡ãƒã‚§ãƒƒã‚¯
            title_hash = hashlib.md5(article['title'][:30].encode()).hexdigest()
            
            if title_hash not in seen_titles:
                unique_news.append(article)
                seen_titles.add(title_hash)
        
        return unique_news
    
    def _score_and_sort(self, news_list: List[Dict]) -> List[Dict]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡è¦åº¦ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¦ä¸¦ã¹æ›¿ãˆ"""
        for article in news_list:
            score = 0
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã«é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
            important_keywords = ['ChatGPT', 'GPT-4', 'Gemini', 'Claude', 'æœ€æ–°', 'ç™ºè¡¨', 'æ–°æ©Ÿèƒ½', 'é©æ–°']
            for keyword in important_keywords:
                if keyword in article['title']:
                    score += 10
            
            # ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹ã‹
            trusted_sources = ['Google News', 'ITmedia', 'ASCII']
            if any(source in article['source'] for source in trusted_sources):
                score += 5
            
            # æ¦‚è¦ã®é•·ã•ï¼ˆæƒ…å ±é‡ï¼‰
            if len(article['summary']) > 100:
                score += 3
            
            # URLãŒã‚ã‚‹ã‹
            if article['url'] and article['url'].startswith('http'):
                score += 2
            
            article['score'] = score
        
        # ã‚¹ã‚³ã‚¢ã®é«˜ã„é †ã«ä¸¦ã¹æ›¿ãˆ
        return sorted(news_list, key=lambda x: (x['score'], x['published']), reverse=True)


# requirements.txtã«è¿½åŠ ãŒå¿…è¦ãªä¾å­˜é–¢ä¿‚
REQUIREMENTS_UPDATE = """
# æ—¢å­˜ã®ä¾å­˜é–¢ä¿‚ã«åŠ ãˆã¦ä»¥ä¸‹ã‚’è¿½åŠ :
feedparser==6.0.10  # RSS ãƒ•ã‚£ãƒ¼ãƒ‰è§£æç”¨
"""
class SimpleNewsCollector(EnhancedNewsCollector):
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def get_ai_news(self, limit: int = 3) -> List[Dict]:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›"""
        # å¼·åŒ–ç‰ˆã®æ©Ÿèƒ½ã‚’ä½¿ã„ã¤ã¤ã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ç¶­æŒ
        return super().get_ai_news(limit=limit, hours_back=48)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ§ª ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    collector = EnhancedNewsCollector()
    news = collector.get_ai_news(limit=5)
    
    print(f"\nğŸ“° åé›†ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(news)}ä»¶")
    print("=" * 50)
    
    for i, article in enumerate(news, 1):
        print(f"\n{i}. {article['title']}")
        print(f"   ğŸ“ ã‚½ãƒ¼ã‚¹: {article['source']}")
        print(f"   ğŸ”— URL: {article['url'][:50]}...")
        print(f"   ğŸ“ æ¦‚è¦: {article['summary'][:100]}...")
        print(f"   â­ ã‚¹ã‚³ã‚¢: {article['score']}")
        print(f"   ğŸ“… æ—¥æ™‚: {article['published'][:19]}")
